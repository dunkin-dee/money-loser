{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218036ad",
   "metadata": {},
   "source": [
    "# Initiation of Table\n",
    "\n",
    "1. Import Table\n",
    "\n",
    "<code>import pandas as pd\n",
    "import numpy as np\n",
    "import rex\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(\"mysql+pymysql://rex:#Pass123@localhost/rex_history\")\n",
    "sql = \"SELECT * FROM `gbpusd_h` ORDER BY `index` ASC\"\n",
    "df = pd.read_sql(sql, engine, index_col=\"index\")</code>\n",
    "\n",
    "2. Drop `volume` column `df.drop(\"volume\", axis=1, inplace=True)`\n",
    "\n",
    "2. Add `wick` and `shadow` columns\n",
    "\n",
    "<code>df[\"shadow\"] = np.where(df[\"direction\"]==\"up\", df[\"open\"]-df[\"low\"], df[\"close\"]-df[\"low\"])\n",
    "df[\"wick\"] = np.where(df[\"direction\"]==\"up\", df[\"high\"]-df[\"close\"], df[\"high\"]-df[\"open\"])</code>\n",
    "\n",
    "*complete the next 2 stops only if read_csv()*\n",
    "3. Set index to datetime(`index`) column  - `df.set_index(\"index\", inplace=True)`\n",
    "4. Convert datetime strings to datetime objects - `df.index = pd.to_datetime(df.index, format ='%Y-%m-%d %H:%M:%S')`\n",
    "\n",
    "## Adding single and double candlestick pattern indicators\n",
    "\n",
    "5. Get single patterns\n",
    "\n",
    "<code>singles = rex.get_single_candlestick_patterns(df)\n",
    "all_singles = []\n",
    "for x in singles.keys():\n",
    "    all_singles = all_singles + singles[x]</code>\n",
    "    \n",
    "6. Remove duplicates, create series for all_singles and add to dataframe (*Series steps needs optimisation*)\n",
    "\n",
    "<code>all_singles = set(all_singles)\n",
    "all_singles = sorted(list(all_singles))\n",
    "singles_series = pd.Series(dtype=\"float\")\n",
    "for x in range(len(df)):\n",
    "    if df.iloc[x].name in all_singles:\n",
    "        singles_series.loc[df.iloc[x].name] = 1\n",
    "    else:\n",
    "        singles_series.loc[df.iloc[x].name] = 0\n",
    "df.insert(len(df.columns), \"singles\", singles_series)</code>\n",
    "\n",
    "7. Repeat last 2 steps for double stick patterns\n",
    "\n",
    "<code>doubles = rex.get_double_candlestick_patterns(df)\n",
    "bull_doubles = doubles[\"bullish\"]\n",
    "bear_doubles = doubles[\"bearish\"]\n",
    "all_bull_doubles = []\n",
    "all_bear_doubles = []\n",
    "for x in bull_doubles.keys():\n",
    "    all_bull_doubles = all_bull_doubles + bull_doubles[x]\n",
    "all_bull_doubles = set(all_bull_doubles)\n",
    "all_bull_doubles = sorted(list(all_bull_doubles))\n",
    "bull_doubles_series = pd.Series(dtype=\"float\")\n",
    "for x in range(len(df)):\n",
    "    if df.iloc[x].name in all_bull_doubles:\n",
    "        bull_doubles_series.loc[df.iloc[x].name] = 1\n",
    "    else:\n",
    "        bull_doubles_series.loc[df.iloc[x].name] = 0\n",
    "df.insert(len(df.columns), \"bull_doubles\", bull_doubles_series)\n",
    "for x in bear_doubles.keys():\n",
    "    all_bear_doubles = all_bear_doubles + bear_doubles[x]\n",
    "all_bear_doubles = set(all_bear_doubles)\n",
    "all_bear_doubles = sorted(list(all_bear_doubles))\n",
    "bear_doubles_series = pd.Series(dtype=\"float\")\n",
    "for x in range(len(df)):\n",
    "    if df.iloc[x].name in all_bear_doubles:\n",
    "        bear_doubles_series.loc[df.iloc[x].name] = 1\n",
    "    else:\n",
    "        bear_doubles_series.loc[df.iloc[x].name] = 0\n",
    "df.insert(len(df.columns), \"bear_doubles\", bear_doubles_series)</code>\n",
    "\n",
    "8. Get support and resistance levels (Use pivot points and turning points)\n",
    "*Turning Points(The previous 5 turning points)*\n",
    "<code>\n",
    "tp = rex.get_turns(df)\n",
    "tp_keys = sorted(list(tp.keys()))\n",
    "level_dict = {}\n",
    "for index, row in df.iterrows():\n",
    "    stop_point = tp_keys[0]\n",
    "    stop_point = next((x for x in tp_keys if x > index ), None)\n",
    "    if stop_point:\n",
    "        tp_index = tp_keys.index(stop_point)\n",
    "        if tp_index > 4:\n",
    "            level_list = []\n",
    "            working_keys = tp_keys[tp_index-5:tp_index]\n",
    "            for some_key in working_keys:\n",
    "                #getting the 5 levels, dependent on movement of turn\n",
    "                if tp[some_key] == \"up\":\n",
    "                    level_list.append(min([float(df.loc[some_key, \"open\"]), float(df.loc[some_key, \"close\"])]))\n",
    "                else:\n",
    "                    level_list.append(max([float(df.loc[some_key, \"open\"]), float(df.loc[some_key, \"close\"])]))\n",
    "            level_dict[index] = {\"level_1\": level_list[0],\n",
    "                                 \"level_2\": level_list[1],\n",
    "                                 \"level_3\": level_list[2],\n",
    "                                 \"level_4\": level_list[3],\n",
    "                                 \"level_5\": level_list[4]}\n",
    "    else:\n",
    "        level_list = []\n",
    "        working_keys = tp_keys[-5:]\n",
    "        for some_key in working_keys:\n",
    "            #getting the 5 levels, dependent on movement of turn\n",
    "            if tp[some_key] == \"up\":\n",
    "                level_list.append(min([float(df.loc[some_key, \"open\"]), float(df.loc[some_key, \"close\"])]))\n",
    "            else:\n",
    "                level_list.append(max([float(df.loc[some_key, \"open\"]), float(df.loc[some_key, \"close\"])]))\n",
    "        level_dict[index] = {\"level_1\": level_list[0],\n",
    "                             \"level_2\": level_list[1],\n",
    "                             \"level_3\": level_list[2],\n",
    "                             \"level_4\": level_list[3],\n",
    "                             \"level_5\": level_list[4]}\n",
    "tp_levels_df = pd.DataFrame.from_dict(level_dict, orient=\"index\")\n",
    "df = pd.concat([tp_levels_df, pp_df], axis=1</code>\n",
    "\n",
    "\n",
    "*From Pivot Points*\n",
    "\n",
    "\n",
    "<code>from sqlalchemy import create_engine\n",
    "engine = create_engine(\"mysql+pymysql://rex:#Pass123@localhost/rex_history\")\n",
    "sql = \"SELECT * FROM `gbpusd_d` ORDER BY `index` ASC\"\n",
    "df_d = pd.read_sql(sql, engine, index_col=\"index\")\n",
    "daily_keys = sorted(list(df_d.index))\n",
    "pp_dict = {}\n",
    "for index, row in df.iterrows():\n",
    "    today_index = daily_keys.index(pd.Timestamp(pd.to_datetime(index).date()))\n",
    "    if today_index > 0:\n",
    "        yesterday = daily_keys[today_index-1]\n",
    "        yesterdict = {\"open\": df_d.loc[yesterday, \"open\"],\n",
    "                      \"high\": df_d.loc[yesterday, \"high\"],\n",
    "                      \"low\": df_d.loc[yesterday, \"low\"],\n",
    "                      \"close\": df_d.loc[yesterday, \"close\"]}\n",
    "        all_pp = rex.get_pp_daily(yesterdict)\n",
    "        pp_dict[index] = {\"pp\" : all_pp[\"daily\"][\"standard\"][\"pp\"],\n",
    "                          \"standard_r1\" : all_pp[\"daily\"][\"standard\"][\"r1\"],\n",
    "                          \"standard_s1\" : all_pp[\"daily\"][\"standard\"][\"s1\"],\n",
    "                          \"fib_r1\" : all_pp[\"daily\"][\"fibonacci\"][\"r1\"],\n",
    "                          \"fib_s1\" : all_pp[\"daily\"][\"fibonacci\"][\"s1\"],}\n",
    "pp_df = pd.DataFrame.from_dict(pp_dict,orient='index')\n",
    "df = pd.concat([df, pp_df], axis=1)</code>\n",
    "\n",
    "9. Add moving averages(sma and ema, both for 5 and 10 periods) *ema needs optimisation*\n",
    "\n",
    "<code>for ma in [5 ,10]:\n",
    "    sma = rex.get_sma(df, ma)\n",
    "    ema = rex.get_ema(df, ma)\n",
    "    df = pd.concat([df, pd.Series(sma).to_frame(f\"sma{ma}\"), pd.Series(ema).to_frame(f\"ema{ma}\")], axis=1)</code>\n",
    "\n",
    "\n",
    "10. Create function for adding dictionaries to dataframe\n",
    "\n",
    "<code>def add_dict_to_df(base_df, my_dict, prefix):\n",
    "    \"\"\"\n",
    "    takes the dataframe to be added to, the dictionary to be added and prefix for column names and returns dataframe with added\n",
    "    columns\n",
    "    \"\"\"\n",
    "    my_df =  pd.DataFrame.from_dict(my_dict, orient=\"index\")\n",
    "    for col_name in my_df.columns:\n",
    "        my_df.rename({col_name: prefix+col_name}, axis=1, inplace=True)\n",
    "    return(pd.concat([base_df, my_df], axis=1))</code>\n",
    "\n",
    "11. Add Bollinger Bands\n",
    "\n",
    "<code>bb = rex.get_bollinger(df)\n",
    "df = add_dict_to_df(df, bb, \"bb_\")</code>\n",
    "\n",
    "12. Add Keltner Channels\n",
    "\n",
    "<code>kelt = rex.get_keltner(df)\n",
    "df = add_dict_to_df(df, kelt, \"kelt_\")</code>\n",
    "\n",
    "13. Add MACD *might skip this step, macd not accurate*\n",
    "\n",
    "<code>macd = rex.get_macd(df)\n",
    "df = add_dict_to_df(df, macd, \"macd_\")</code>\n",
    "    \n",
    "14. Add RSI\n",
    "\n",
    "<code>rsi = rex.get_rsi(df)\n",
    "df = pd.concat([df, pd.Series(rsi).to_frame(\"rsi\")], axis=1)</code>\n",
    "\n",
    "15. Add Parabolic SAR\n",
    "\n",
    "<code>psar = rex.get_parabolic_sar(df)\n",
    "df = add_dict_to_df(df, psar, \"psar_\")</code>\n",
    "\n",
    "16. Add Stochastic Indicator\n",
    "\n",
    "<code>stoch = rex.get_stochastic(df)\n",
    "df = add_dict_to_df(df, stoch, \"stoch_\")</code>\n",
    "\n",
    "17. Add ADX\n",
    "\n",
    "<code>adx = rex.get_adx(df)\n",
    "df = add_dict_to_df(df, adx, \"adx_\")</code>\n",
    "\n",
    "18. Add Williams %R\n",
    "\n",
    "<code>will = rex.get_williams_r(df)\n",
    "df = pd.concat([df, pd.Series(will).to_frame(\"williams\")], axis=1)</code>\n",
    "\n",
    "\n",
    "20. Add row indicating the direction since the previous turn(adjust forward by length used to calculate the turns)\n",
    "\n",
    "<code> turns = rex.get_turns(df)\n",
    "turns_keys = sorted(list(turns.keys()))\n",
    "turns_series = pd.Series(dtype=\"object\")\n",
    "for index, row in df.iterrows():\n",
    "    stop_time = next((x for x in turns_keys if x > index), None)\n",
    "    if stop_time:\n",
    "        future_index = turns_keys.index(stop_time)\n",
    "        if future_index > 0:\n",
    "            use_time = turns_keys[future_index-1]\n",
    "            turns_series.loc[index] = turns[use_time]\n",
    "    else:\n",
    "        turns_series[index] = turns[turns_keys[-1]]\n",
    "df = pd.concat([df, turns_series.to_frame('curr_trend')], axis=1)</code>\n",
    "\n",
    "\n",
    "21. Use OneHotEncoder/get_dummies to categorise direction, trend and psar_direction\n",
    "\n",
    "<code>df = pd.concat([df.drop([\"direction\", \"curr_trend\", \"psar_direction\"], axis=1), \n",
    "                      pd.get_dummies(df[[\"direction\", \"curr_trend\", \"psar_direction\"]])], axis=1)</code>\n",
    "                      \n",
    "\n",
    "\n",
    "22. Add previous rows to current row(5 times)\n",
    "\n",
    "<code>shifted_df= {}\n",
    "for x in range(1, 6):\n",
    "    temp_df = df[df.columns].shift(x)\n",
    "    for col_name in temp_df.columns:\n",
    "        temp_df.rename({col_name: col_name+f\"-{str(x)}\"}, axis=1, inplace=True)\n",
    "    shifted_df[x] = temp_df\n",
    "for x in shifted_df.keys():\n",
    "    df = pd.concat([df, shifted_df[x]], axis=1)</code>\n",
    "    \n",
    "    \n",
    "    \n",
    "23. Determine if row leads to positive pips\n",
    "\n",
    "<code>df_keys = sorted(list(df.index))\n",
    "def getfirst_pandas(condition, df):\n",
    "    cond = df[condition(df)]\n",
    "    if not cond.empty:\n",
    "        return(cond.iloc[0].name)\n",
    "    else:\n",
    "        return None\n",
    "df[\"target\"] = np.zeros(len(df))\n",
    "for index, row in df.iterrows():\n",
    "    base_point = row[\"close\"]\n",
    "    goal_point = base_point + 0.0035\n",
    "    fail_point = base_point - 0.0020\n",
    "    small_df = df[df_keys.index(index)+1:df_keys.index(index)+16]\n",
    "    small_keys = sorted(list(small_df.index))\n",
    "    goal_reached = getfirst_pandas(lambda x: x.high >= goal_point, small_df)\n",
    "    if goal_reached:\n",
    "        goal_index = small_keys.index(goal_reached)\n",
    "        if small_df[:goal_index][\"low\"].min() > fail_point:\n",
    "            df.at[index, 'target'] = 1</code>\n",
    "        \n",
    "24. Drop all rows with NaN and drop last 15 rows\n",
    "\n",
    "<code>df.dropna(inplace=True)\n",
    "df = df[:-15]</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e3af9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
